{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45e80c87",
      "metadata": {
        "id": "45e80c87"
      },
      "source": [
        "<h1>MICRODATA PROTECTION</h1>\n",
        "\n",
        "Snippets of code to protect your micordata. To make the code work, you will also need adult and iris datasets in the same directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92260ba5",
      "metadata": {
        "id": "92260ba5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "\n",
        "from scipy.linalg import cholesky\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by importing the adult dataset. This dataset contains information about the income of a pool of 32561 adults, together with some demographic details, such as their age, workclass, education, number of years worked per hour.\n",
        "\n",
        "check <url>https://www.kaggle.com/datasets/wenruliu/adult-income-dataset</url> to understand what each variable corresponds to."
      ],
      "metadata": {
        "id": "3q7MogfwgF00"
      },
      "id": "3q7MogfwgF00"
    },
    {
      "cell_type": "code",
      "source": [
        "adult_data_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "\n",
        "adult_columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
        "       'income']\n",
        "\n",
        "adultdf = pd.read_csv(adult_data_path, sep=\",\", names = adult_columns)\n",
        "adultdf = adultdf.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "adultdf.head()"
      ],
      "metadata": {
        "id": "rf9J-sF1fhHl"
      },
      "id": "rf9J-sF1fhHl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1419cc80",
      "metadata": {
        "id": "1419cc80"
      },
      "source": [
        "When it comes to analyzing a dataset from the privacy perspective, the first thing that we need to do, is to visualize the content of our data. As we discussed during the lecture, one of the main \"challenges\" when it comes to privacy protection concerns protecting the \"outliers\": they have peculiar characteristics that make them to stand out. If we want to protect them, we first need to identify them by looking at the distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e91c0d",
      "metadata": {
        "id": "16e91c0d"
      },
      "outputs": [],
      "source": [
        "#hours-per-week is an interesting attribute. How do you expect it to distribute\n",
        "#in the general population? How do you expect it to correlate with the income?\n",
        "#How a malicious adversarial can exploit this information to damage someone?\n",
        "\n",
        "#lets start by plotting the histogram of hours per week.\n",
        "#if you do not know where to start, check: https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
        "\n",
        "#here goes your line of code\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74bd3784",
      "metadata": {
        "id": "74bd3784"
      },
      "source": [
        "# MACRODATA\n",
        "\n",
        "We start our exercitation testing the obfuscation techniques that we have learned to protect **macrodata** releases. We recall that a macrodata release is a release of data in aggregated form, often presented as as double entry table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86a0fe9",
      "metadata": {
        "id": "c86a0fe9"
      },
      "outputs": [],
      "source": [
        "#convert sex/occupation columns in a double entry table\n",
        "\n",
        "#count the occurrences for each pair (sex, occupation)\n",
        "#reset the index, just to apply the next step\n",
        "#rotate the table to look like a double entry table\n",
        "#replace \"na\" values (i.e., (sex, occupation) pairs not existing) with 0\n",
        "macrodata_occupation = adultdf[['sex', 'occupation']].groupby('sex').value_counts()\\\n",
        "                         .reset_index()\\\n",
        "                         .pivot_table(index='occupation', columns='sex', values='count')\\\n",
        "                         .fillna(0)\n",
        "\n",
        "#print the table\n",
        "macrodata_occupation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7444228",
      "metadata": {
        "id": "b7444228"
      },
      "source": [
        "## Threshold Rule\n",
        "\n",
        "What cells are sensible? Which threshold should we use? If our threshold is 500, we will have to remove all the values corresponding to cells with less than 500 subjects before releasing our data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we first copy the dataset to avoid overwriting it\n",
        "privatized_mo = macrodata_occupation.copy()\n",
        "\n",
        "threshold = 500\n",
        "\n",
        "#we need to identify which cells to obfuscate according to the threshold rule\n",
        "\n",
        "#if you are unfamiliar with pandas indexing, check: https://pandas.pydata.org/docs/user_guide/indexing.html\n",
        "#put here your line of code, it should look something like\n",
        "\n",
        "privatized_mo[<which cells do we need to remove?>] = \"SUPPRESSED\"\n",
        "\n",
        "\n",
        "privatized_mo"
      ],
      "metadata": {
        "id": "GCtskvAmAKu3"
      },
      "id": "GCtskvAmAKu3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c2a504db",
      "metadata": {
        "id": "c2a504db"
      },
      "source": [
        "*SOME SUGGESTIONS FOR HW1*</br>\n",
        "Show on your data how to carry out:\n",
        "<ul>\n",
        "    <li> cell suppression </li>\n",
        "    <li> rounding </li>\n",
        "    <li> roll up categories </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "402922d6",
      "metadata": {
        "id": "402922d6"
      },
      "source": [
        "## nk-rule\n",
        "\n",
        "to determine which cells are sensitive using the n-k rule, we need to determine how much each individual is responsible for the value of the cell.\n",
        "\n",
        "\n",
        "\n",
        "> nk-rule: a cell is considered sensitive according to the nk rule if **less than n** individuals contributed more to the value of the cell for **more than k% of the total**.\n",
        "\n",
        "\n",
        "\n",
        "The aggregation is the sum: what is the contribution of each individual?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b80e110",
      "metadata": {
        "id": "1b80e110"
      },
      "outputs": [],
      "source": [
        "def nkrule_sum(values: np.array, n: int = 3, k: float = 0.3):\n",
        "    \"\"\"\n",
        "    ## Function: nkrule\n",
        "\n",
        "    Determines if the number of individuals contributing more than a specified\n",
        "    fraction `k` is less than a threshold `n`.\n",
        "\n",
        "    ### Parameters\n",
        "\n",
        "    - `values` (`np.array`): Array of numerical values representing contributions.\n",
        "    - `n` (`int`, default=3): Threshold number of individuals.\n",
        "    - `k` (`float`, default=0.3): Contribution fraction threshold.\n",
        "\n",
        "    ### Returns\n",
        "\n",
        "    - `bool`: `True` if the number of contributors with contribution > `k` is less than `n`, otherwise `False`.\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "    #what is the contribution of the different individuals for the sum aggregation?\n",
        "    contribution = ???\n",
        "    print(f\"contributions: {contribution}\")\n",
        "    print(f\"number of individuals contributing more than {k}: {np.sum(contribution > k)}\")\n",
        "\n",
        "    #what is the condition that tells us if the cell is sensitive?\n",
        "    is_sensitive = ???\n",
        "    return is_sensitive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following scenario: we are trying to compute the time spent in studying for the various cybersecurity courses during the week.\n",
        "\n",
        "We report the results on a table with a cell for each course.\n",
        "\n",
        "we are interested in determining if, according to the 3-0.3 rule, the cell for the PPIA course is sensitive.\n",
        "\n",
        "Alice spends 20 hours studying PPIA, Bob and Claire both spend 10 hours, David spend 0 hours.  "
      ],
      "metadata": {
        "id": "QOE01HECGaxt"
      },
      "id": "QOE01HECGaxt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8050f2b3",
      "metadata": {
        "id": "8050f2b3"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "k = 0.3\n",
        "\n",
        "print(f\"is the cell sensible according to {n}-{k} rule?: {nkrule_sum(np.array([20, 10, 10, 0]), n, k)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alice differs too much from the \"standard student\": this cell is much more informative on her behaviour than others'."
      ],
      "metadata": {
        "id": "yersKBOSC2H5"
      },
      "id": "yersKBOSC2H5"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Now we slightly modify the scenario. and we assume that also Alice spends 10 hours\n",
        "studying, making her more similar to the rest of the students.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"is the cell sensible according to {n}-{k} rule?: {nkrule_sum(np.array([10, 10, 10, 0]), n, k)}\\n\")"
      ],
      "metadata": {
        "id": "07wQ_rqICqUx"
      },
      "id": "07wQ_rqICqUx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We go back to the adult dataset and we are interested in verifying if the cell \"other\"-\"priv-house-serv\" is sensitive (i.e., contains subjects that differ) too much from the average"
      ],
      "metadata": {
        "id": "FqqsY9_VHSz5"
      },
      "id": "FqqsY9_VHSz5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219f6156",
      "metadata": {
        "id": "219f6156"
      },
      "outputs": [],
      "source": [
        "macrodata_hpw = adultdf.groupby([\"race\", \"occupation\"])[\"hours-per-week\"].mean().reset_index()\\\n",
        "                       .pivot_table(index='race', columns='occupation', values='hours-per-week')\\\n",
        "                       .fillna(0)\n",
        "\n",
        "macrodata_hpw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "490a4bda",
      "metadata": {
        "id": "490a4bda"
      },
      "outputs": [],
      "source": [
        "#lets choose a cell that we think it might be  sensitive: sensitive cells are\n",
        "#often those that have few contributors and with a skwed distribution\n",
        "contr = adultdf[(adultdf['race']=='Asian-Pac-Islander') & (adultdf['occupation']=='Priv-house-serv')]['hours-per-week'].to_numpy()\n",
        "\n",
        "\n",
        "n = 3; k = 0.20\n",
        "print(f\"is the cell sensible according to {n}-{k} rule?: {nkrule_sum(contr, n, k)}\\n\")\n",
        "\n",
        "#if we increase k, are we increasing or decreasing the privacy requirements?\n",
        "#what about n?\n",
        "k = 0.22\n",
        "print(f\"is the cell sensible according to {n}-{k} rule?: {nkrule_sum(contr, n, k)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1dd1d22",
      "metadata": {
        "id": "c1dd1d22"
      },
      "source": [
        "*SOME SUGGESTIONS FOR HW1*</br>\n",
        "Show, on your data or on the adult dataset which cells are sensible according to:\n",
        "<ul>\n",
        "    <li> p-percentage </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd0b1fa",
      "metadata": {
        "id": "3dd0b1fa"
      },
      "source": [
        "# MICRODATA\n",
        "\n",
        "A microdata release occurs when data are released at the level of individual respondents. Because they are more fine-grained, microdata often retain higher utility compared to macrodata, as they allow for more detailed analyses - even for analysts who did not have access to the original data. <br/>\n",
        "At the same time, the increased utility of microdata, due to the natural protection-utility trade-off inherent in privacy protection, results in a higher risk of re-identification or data leakage.\n",
        "Consequently, stronger approaches should be considered to protect the data.\n",
        "\n",
        "## MICRODATA PROTECTION APPROACHES: MASKING\n",
        "Approaches based on masking operate by hiding and scrambling part of the information or rare combinations of attribute's values to reduce the risk of reidentification and protect the privacy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6d4ddb",
      "metadata": {
        "id": "bd6d4ddb"
      },
      "source": [
        "### Sampling\n",
        "\n",
        "Sampling is an example of masking-based microdata-protrection strategy that relies on randomly sampling part of the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c2e724e",
      "metadata": {
        "id": "4c2e724e"
      },
      "outputs": [],
      "source": [
        "def sampling(dataset: pd.DataFrame, sample_size:float=0.5):\n",
        "  \"\"\"\n",
        "  Samples a specified proportion of individuals from a dataset.\n",
        "\n",
        "  Args:\n",
        "      dataset (pd.DataFrame): The input pandas DataFrame.\n",
        "      sample_size (float): The proportion of individuals to sample.\n",
        "                           Defaults to 0.5 (50%).\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: A new DataFrame containing the sampled individuals.\n",
        "  \"\"\"\n",
        "  #compute the number of individuals in the dataset\n",
        "  n_idividuals = ???\n",
        "  #compute the number of individuals to be sampled\n",
        "  n_sampled_individuals = ???\n",
        "  #sample the individuals. If you do not know where to start, check: https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
        "  sampled_individuals = ???\n",
        "  #keep only sampled individuals\n",
        "  sampled_dataset = ???\n",
        "  return sampled_dataset\n",
        "\n",
        "sampled_adultdf = sampling(adultdf, sample_size=0.3)\n",
        "\n",
        "print(f\"the original dataset had {len(adultdf)} records, the sampled one has {len(sampled_adultdf)} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccbd1986",
      "metadata": {
        "id": "ccbd1986"
      },
      "source": [
        "What challenges are associated with the sampling strategy?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1192312e",
      "metadata": {
        "id": "1192312e"
      },
      "source": [
        "<h3> Local Suppression </h3>\n",
        "\n",
        "Local suppression is based on identifying individuals or cells whose value we consider sensitive and remove it from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8ae4c2",
      "metadata": {
        "id": "7f8ae4c2"
      },
      "source": [
        "In this example, we considered the number of hours worked per week of individuals whose race is 'race'=='Asian-Pac-Islander' and whose occupation is 'occupation'=='Priv-house-serv' as potentially sensible. If the number of hours worked per week is high, we remove the information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ee4089",
      "metadata": {
        "id": "00ee4089"
      },
      "outputs": [],
      "source": [
        "tmp_adultdf = adultdf.copy()\n",
        "\n",
        "tmp_adultdf.query(\"race == 'Asian-Pac-Islander' and occupation == 'Priv-house-serv'\")['hours-per-week']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a96a08c",
      "metadata": {
        "id": "6a96a08c"
      },
      "source": [
        "more in details, we consider problematic cells for which the number of hours per week is above 50: individuals that spend most of the time outside their house and that are likely forced to unsatisfactory work conditions due to a low income. We will remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40b902d2",
      "metadata": {
        "id": "40b902d2"
      },
      "outputs": [],
      "source": [
        "tmp_adultdf.query(\"race == 'Asian-Pac-Islander' and occupation == 'Priv-house-serv' and `hours-per-week`>50\")['hours-per-week']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a85831",
      "metadata": {
        "id": "66a85831"
      },
      "outputs": [],
      "source": [
        "#we use np.nan as suppessing value to maintain the data type of the column (floats)\n",
        "#how to change the value of the sensible cells?\n",
        "\n",
        "\n",
        "tmp_adultdf.loc[<what is the index of sensitive cells?>, 'hours-per-week'] = np.nan\n",
        "\n",
        "tmp_adultdf.query(\"race == 'Asian-Pac-Islander' and occupation == 'Priv-house-serv'\")['hours-per-week']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a07558e0",
      "metadata": {
        "id": "a07558e0"
      },
      "source": [
        "Notice that, if we use these data to compute a macrodata release, it is likely that the cell will remain sensible due to the fact that only two subjects contribute to 50% of its value: we have to verify that and, eventually, suppress other values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5a91904",
      "metadata": {
        "id": "f5a91904"
      },
      "source": [
        "### Global Recoding\n",
        "\n",
        "Global recoding is a microdata protection technique based on replacing the values within an attribute with a more general \"code\". Typically, the domain of the attribute that we wish to protect is partitioned in equal-sized bins and the values that fall in a bin are replaced with the interval describing the bin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a792ec32",
      "metadata": {
        "id": "a792ec32"
      },
      "outputs": [],
      "source": [
        "def global_recoding(input_series: pd.Series, labels: int | list=5):\n",
        "  \"\"\"\n",
        "  Applies global recoding to a pandas Series by partitioning values into bins.\n",
        "\n",
        "  Args:\n",
        "      input_series (pd.Series): The input pandas Series to be recoded.\n",
        "      labels (int | list): If an integer, it specifies the number of equal-sized bins.\n",
        "                           If a list, it specifies the labels for the bins.\n",
        "                           Defaults to 5.\n",
        "\n",
        "  Returns:\n",
        "      pd.Series: A new pandas Series with values replaced by their corresponding bin labels.\n",
        "\n",
        "  Raises:\n",
        "      ValueError: If `labels` is neither an integer nor a list.\n",
        "  \"\"\"\n",
        "  #equal sized bins\n",
        "  if type(labels)==list:\n",
        "    #here, you should remap the input_series values into the new values (considering you have received the labels to use from outside)\n",
        "    #if you do not know where to start, check https://pandas.pydata.org/docs/reference/api/pandas.cut.html\n",
        "    output_series = ???\n",
        "  elif type(labels)==int:\n",
        "    output_series = ???\n",
        "  else:\n",
        "    raise ValueError(\"labels must be a list or an integer\")\n",
        "  return output_series\n",
        "\n",
        "recoded_adultdf = adultdf.copy()\n",
        "recoded_adultdf[\"obfuscated_hpw\"] = global_recoding(recoded_adultdf[\"hours-per-week\"])\n",
        "print(recoded_adultdf[\"obfuscated_hpw\"].unique())\n",
        "recoded_adultdf[['hours-per-week', 'obfuscated_hpw']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6df4f7ae",
      "metadata": {
        "id": "6df4f7ae"
      },
      "outputs": [],
      "source": [
        "recoded_adultdf[\"obfuscated_hpw\"] = global_recoding(recoded_adultdf[\"hours-per-week\"], labels=[\"very low\", \"low\", \"normal\", \"high\", \"very high\"])\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # 1 row, 2 columns\n",
        "\n",
        "# Left plot: histogram of 'hours-per-week'\n",
        "sns.histplot(data=recoded_adultdf, x='hours-per-week', ax=axes[0])\n",
        "axes[0].set_title('Hours per Week')\n",
        "\n",
        "# Right plot: histogram of 'obfuscated_hpw'\n",
        "sns.histplot(data=recoded_adultdf, x='obfuscated_hpw', ax=axes[1])\n",
        "axes[1].set_title('Obfuscated Hours per Week')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11572872",
      "metadata": {
        "id": "11572872"
      },
      "outputs": [],
      "source": [
        "#alternatively, you can set the number of individuals per bin, instead of\n",
        "#setting the number of bins. How do privacy risks change? What about utility?\n",
        "\n",
        "#quantized bins\n",
        "\n",
        "adultdf['hpw'] = pd.qcut(adultdf['hours-per-week'], 10, duplicates='drop')\n",
        "adultdf[['hours-per-week', 'hpw']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9373d28",
      "metadata": {
        "id": "b9373d28"
      },
      "source": [
        "What problems present the global recoding? are all the bins equal in size? do all the bins contain the same number of subjects?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69993125",
      "metadata": {
        "id": "69993125"
      },
      "source": [
        "<h3>Top & Bottom Coding</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e7cc99e",
      "metadata": {
        "id": "9e7cc99e"
      },
      "outputs": [],
      "source": [
        "def top_coding(input_series: pd.Series, threshold: float, top_code: any):\n",
        "  \"\"\"\n",
        "  Applies top coding to a pandas Series, replacing values above a threshold.\n",
        "\n",
        "  Args:\n",
        "      input_series (pd.Series): The input pandas Series to be top coded.\n",
        "      threshold (float): The threshold value. Values greater than this\n",
        "                         will be replaced.\n",
        "      top_code (any): The value to replace the data above the threshold with.\n",
        "\n",
        "  Returns:\n",
        "      pd.Series: A new pandas Series with top coding applied.\n",
        "  \"\"\"\n",
        "  output_series = input_series.copy()\n",
        "  output_series[<which cells should we obfucate?>] = top_code\n",
        "\n",
        "  return output_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46a5bc3b",
      "metadata": {
        "id": "46a5bc3b"
      },
      "outputs": [],
      "source": [
        "def bottom_coding(input_series: pd.Series, threshold: float, bottom_code: str):\n",
        "  \"\"\"\n",
        "  Applies bottom coding to a pandas Series, replacing values below a threshold.\n",
        "\n",
        "  Args:\n",
        "      input_series (pd.Series): The input pandas Series to be bottom coded.\n",
        "      threshold (float): The threshold value. Values less than this\n",
        "                         will be replaced.\n",
        "      bottom_code (any): The value to replace the data below the threshold with.\n",
        "\n",
        "  Returns:\n",
        "      pd.Series: A new pandas Series with bottom coding applied.\n",
        "  \"\"\"\n",
        "  output_series = input_series.copy()\n",
        "  output_series[<which cells should we obfucate?>] = bottom_code\n",
        "\n",
        "  return output_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e20c75",
      "metadata": {
        "id": "b5e20c75"
      },
      "outputs": [],
      "source": [
        "\n",
        "recoded_adultdf[\"topcoded_hpw\"] = top_coding(recoded_adultdf[\"hours-per-week\"], 60, \">60\")\n",
        "recoded_adultdf[\"bottom_hpw\"] = bottom_coding(recoded_adultdf[\"hours-per-week\"], 20, \"<20\")\n",
        "\n",
        "to_be_recoded = (recoded_adultdf[\"hours-per-week\"]>60) | (recoded_adultdf[\"hours-per-week\"]<20)\n",
        "recoded_adultdf[to_be_recoded][['hours-per-week', 'topcoded_hpw', 'bottom_hpw']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13e5c324",
      "metadata": {
        "id": "13e5c324"
      },
      "source": [
        "<h3>Generalization</h3>\n",
        "\n",
        "Generalization is similar to global recoding, but it is applied to categorical attributes. In this case, we need a generalization hierarchy and we replace the value(s) of the attribute with a generalization.\n",
        "\n",
        "Lets start by inspecting the occupation values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193dd318",
      "metadata": {
        "id": "193dd318"
      },
      "outputs": [],
      "source": [
        "adultdf['occupation'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now build the generalization hierarchy, using a python dictionary."
      ],
      "metadata": {
        "id": "e-nX4e9q-vTg"
      },
      "id": "e-nX4e9q-vTg"
    },
    {
      "cell_type": "code",
      "source": [
        "generalization_hierarchy = {\n",
        "    \"manual\": ['Machine-op-inspct', 'Farming-fishing', 'Craft-repair', 'Tech-support',  'Priv-house-serv', 'Handlers-cleaners'],\n",
        "    \"protection\": ['Armed-Forces', 'Protective-serv'],\n",
        "    \"business\": ['Prof-specialty', 'Adm-clerical', 'Exec-managerial', 'Sales', 'Transport-moving'],\n",
        "    \"others-or-unknown\": ['?', 'Other-service']\n",
        "}\n",
        "\n",
        "\n",
        "#for the next bit of code, we also need to reverse the hierarchy, so that\n",
        "#the specific value is mapped to the more general one\n",
        "\n",
        "gen_hier_reversed = {\n",
        "    specific: general\n",
        "    for general, specifics in generalization_hierarchy.items()\n",
        "    for specific in specifics\n",
        "}\n"
      ],
      "metadata": {
        "id": "Ldx2Lcn_-9uL"
      },
      "id": "Ldx2Lcn_-9uL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89cbd430",
      "metadata": {
        "id": "89cbd430"
      },
      "outputs": [],
      "source": [
        "def generalization(input_series: pd.Series, hierarchy: dict):\n",
        "  \"\"\"\n",
        "  Applies generalization to a pandas Series based on a provided hierarchy.\n",
        "\n",
        "  Args:\n",
        "      input_series (pd.Series): The input pandas Series to be generalized.\n",
        "      hierarchy (dict): A dictionary mapping specific values to their\n",
        "                        generalized categories.\n",
        "\n",
        "  Returns:\n",
        "      pd.Series: A new pandas Series with values replaced by their\n",
        "                 generalized categories.\n",
        "  \"\"\"\n",
        "  output_series = input_series.copy()\n",
        "  #here you need to write the line of code to remap the series using the hierarchy.\n",
        "  #if you do not know where to start, check https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.map.html\n",
        "  output_series = ???\n",
        "  return output_series\n",
        "\n",
        "recoded_adultdf['occ_generalized'] = generalization(recoded_adultdf[\"occupation\"], gen_hier_reversed)\n",
        "recoded_adultdf[['occupation', 'occ_generalized']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36467264",
      "metadata": {
        "id": "36467264"
      },
      "source": [
        "<h3>Resampling</h3>\n",
        "\n",
        "The resampling approach is a masking approach based on bootstrapping (to check the basics about bootstrapping a good book is: https://www.hms.harvard.edu/bss/neuro/bornlab/nb204/statistics/bootstrap.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf1a9a75",
      "metadata": {
        "id": "cf1a9a75"
      },
      "outputs": [],
      "source": [
        "#slides example\n",
        "M = np.array([10, 18, 20, 8, 11, 14])\n",
        "print(f\"original values: {M}\")\n",
        "\n",
        "print(f\"argsort: {np.argsort(M)}\")\n",
        "\n",
        "sM = np.array([npr.choice(M, size=len(M)) for s in range(4)]).T\n",
        "print(\"\\nsampled table:\")\n",
        "print(sM)\n",
        "\n",
        "rows, cols = sM.shape\n",
        "\n",
        "sMs = np.array([sorted(sM[:, c]) for c in range(cols)]).T\n",
        "print(\"\\nsorted table:\")\n",
        "print(sMs)\n",
        "\n",
        "means = np.mean(sMs, axis=1)\n",
        "print(f\"\\nmeans: {means}\")\n",
        "released = np.zeros(len(M))\n",
        "for e, i in enumerate(np.argsort(M)):\n",
        "    released[i] = means[e]\n",
        "print(f\"released values: {released}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4569c2a4",
      "metadata": {
        "id": "4569c2a4"
      },
      "outputs": [],
      "source": [
        "def resampling(values, repetitions: int=4):\n",
        "  #resample the values with replacement (np.random.choice)\n",
        "  sampled_values = ???\n",
        "\n",
        "  #sort the values in each column separately\n",
        "\n",
        "  sorted_sampled_values = ???\n",
        "\n",
        "  #compute the means row-wise\n",
        "  new_values = ???\n",
        "\n",
        "  #now, get the indexes to sort the values\n",
        "  indexes = ???\n",
        "\n",
        "  #we need a support vector where to put the new computed values\n",
        "  out_values = ???\n",
        "\n",
        "  #now we are ready to rearrange the new values\n",
        "  out_values[???] = new_values\n",
        "  return out_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ee0d7e",
      "metadata": {
        "id": "88ee0d7e"
      },
      "outputs": [],
      "source": [
        "values = resampling(adultdf['fnlwgt'].to_numpy(), repetitions=4)\n",
        "print(f\"original: {adultdf['fnlwgt'].to_list()[:10]}\")\n",
        "print(f\"released: {values[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now switch to a different dataset that contains more numeric values and where the variables show some (strong) correlation"
      ],
      "metadata": {
        "id": "HhHCtPRHGTXi"
      },
      "id": "HhHCtPRHGTXi"
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
        "variables = [\"sl\", \"sw\", \"pl\", \"pw\"]\n",
        "iris = pd.read_csv(data_path, names = variables + [\"label\"])\n"
      ],
      "metadata": {
        "id": "RYNGuKr9Fjuk"
      },
      "id": "RYNGuKr9Fjuk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show the strength of such a correlation, we can plot it. Why is the correlation important for us?"
      ],
      "metadata": {
        "id": "AF2b34jfGSTc"
      },
      "id": "AF2b34jfGSTc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093c2332",
      "metadata": {
        "id": "093c2332"
      },
      "outputs": [],
      "source": [
        "corr = iris[variables].corr()\n",
        "sns.heatmap(corr)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "601a5cea",
      "metadata": {
        "id": "601a5cea"
      },
      "source": [
        "<h3>Random noise: uncorrelated additive noise</h3>\n",
        "\n",
        "We now assume to apply the \"random noise\" perturbation technique. In this case, we add some random noise to the numeric values. <br/>\n",
        "The noise can be drawn in two diffrent ways, uncorrelated noise, where noise is sampled independently from a univariate distribtion, or we can draw correlated noise. In this second case, we need to draw noise from a multivariate distribution where the underlying covariance corresponds to the covariance of the original values.\n",
        "\n",
        "\n",
        "Lets start with the uncorrelated noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f6e5a0",
      "metadata": {
        "id": "f7f6e5a0"
      },
      "outputs": [],
      "source": [
        "def uncorrelated_additive_noise(dataset: np.array):\n",
        "  \"\"\"\n",
        "  Adds uncorrelated additive noise to a dataset.\n",
        "\n",
        "  The noise for each variable is drawn from a normal distribution with\n",
        "  mean 0 and a standard deviation equal to the standard deviation of\n",
        "  the corresponding variable in the input dataset.\n",
        "\n",
        "  Args:\n",
        "      dataset (np.array): The input numpy array containing numerical data.\n",
        "\n",
        "  Returns:\n",
        "      np.array: A new numpy array with uncorrelated additive noise applied.\n",
        "  \"\"\"\n",
        "\n",
        "  #compute the variance\n",
        "  sigma2 = ???\n",
        "\n",
        "  # generate the random noisefrom the normal distribution with mean 0 and the\n",
        "  # computed standard deviations.\n",
        "  # we need to generate noise for each instance and for each considered variable:\n",
        "  # what does this tell you about the shape of the noise?\n",
        "  noise = ???\n",
        "\n",
        "  #now, we can add the noise to the dataset\n",
        "  dataset += noise\n",
        "\n",
        "  return dataset\n",
        "\n",
        "noisy_irisdf = iris.copy()\n",
        "noisy_irisdf[variables] = uncorrelated_additive_noise(iris[variables].to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "738ae79f",
      "metadata": {
        "id": "738ae79f"
      },
      "outputs": [],
      "source": [
        "noisy_irisdf[variables].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a023cb86",
      "metadata": {
        "id": "a023cb86"
      },
      "outputs": [],
      "source": [
        "iris[variables].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened to our correlations? What does this mean for our analyses"
      ],
      "metadata": {
        "id": "c6MmqEwKItgq"
      },
      "id": "c6MmqEwKItgq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a5a20e6",
      "metadata": {
        "id": "9a5a20e6"
      },
      "outputs": [],
      "source": [
        "corr = noisy_irisdf[variables].corr()\n",
        "sns.heatmap(corr)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccfc2e49",
      "metadata": {
        "id": "ccfc2e49"
      },
      "source": [
        "<h3>Random noise: correlated additive noise</h3>\n",
        "\n",
        "Uncorrelated noise, unfortunately, resulted in a dataset where all the relations between the variables were lost (severely reduced correlation between variables).\n",
        "\n",
        "We now want to sample correlated additive noise. To do so, we need to sample it from a multivariate distribution where the covariance matrix is the covariance matrix of the original variables.\n",
        "\n",
        "Given two variables $X$ and $Y$, the covariance is defined as follows:\n",
        "\n",
        "$$\\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})$$\n",
        "\n",
        "We do not need to compute it from scratches, and we can rely on operators already available in pandas. In particular, by calling `.cov()` on a pandas dataframe with numeric columns, it is possible to compute the covariance matrix.\n",
        "\n",
        "What does the diagonal of the covariance matrix contain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8adb4068",
      "metadata": {
        "id": "8adb4068"
      },
      "outputs": [],
      "source": [
        "#compute the covariance matrix\n",
        "covm = iris[variables].cov()\n",
        "print(f\"diagonal of the covariance matrix: {np.diag(covm)}\") # the diagonal of the covariance matrix, contains the variance\n",
        "print(f\"variance of the variables: {iris[variables].std().to_numpy()**2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2410400b",
      "metadata": {
        "id": "2410400b"
      },
      "outputs": [],
      "source": [
        "def correlated_additive_noise(dataset: np.array):\n",
        "  \"\"\"\n",
        "  Adds correlated additive noise to a dataset.\n",
        "\n",
        "  The noise is drawn from a multivariate normal distribution centered at 0\n",
        "  with a covariance matrix equal to the covariance matrix of the input dataset.\n",
        "\n",
        "  Args:\n",
        "      dataset (np.array): The input numpy array containing numerical data.\n",
        "\n",
        "  Returns:\n",
        "      np.array: A new numpy array with correlated additive noise applied.\n",
        "  \"\"\"\n",
        "\n",
        "  #compute the covariance matrix. Notice that now the input is a np.array\n",
        "  covm = ???\n",
        "\n",
        "  # now we need to generate the correlated noise. to do so, we can use a\n",
        "  # multivariate normal distribution centered in 0 and with the given covariance\n",
        "  # matrix. Which python function you can use to achieve this?\n",
        "  # IMPORTANT: make sure to sample noise with the right shape: how many values are\n",
        "  # you going to sample every time from a multivariate normal distriubtion?\n",
        "  covnoise = ???\n",
        "\n",
        "\n",
        "  #now, we can add the noise to the dataset\n",
        "  dataset += covnoise\n",
        "\n",
        "  return dataset\n",
        "\n",
        "corr_noisy_irisdf = iris.copy()\n",
        "corr_noisy_irisdf[variables] = correlated_additive_noise(iris[variables].to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about the correlation between variables now?"
      ],
      "metadata": {
        "id": "0Wdh9Z6YMZR-"
      },
      "id": "0Wdh9Z6YMZR-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e3fb930",
      "metadata": {
        "id": "1e3fb930"
      },
      "outputs": [],
      "source": [
        "corr = corr_noisy_irisdf[variables].corr()\n",
        "sns.heatmap(corr)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebc4036d",
      "metadata": {
        "id": "ebc4036d"
      },
      "source": [
        "<h2>MICRODATA: GENERATION</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0bef6ce",
      "metadata": {
        "id": "c0bef6ce"
      },
      "source": [
        "<h3>Cholesky decomposition</h3>\n",
        "\n",
        "The synthetic generation based on the Cholesky decomposition works as follow.\n",
        "Assume $D\\in\\mathbb{R}^{n \\times m}$ is your dataset with $n$ instances and $m$ variables.\n",
        "\n",
        "Let's call $C$ the covariance matrix - we have learned how to compute it with the previous exercise.\n",
        "\n",
        "Using the Cholesky decomposition, we can write $C$ as the product of two matrices:\n",
        "\n",
        "$$C = U^*U$$\n",
        "\n",
        "Where $U^*$ is the conjugate transpose of U.\n",
        "\n",
        "We can now construct $R\\in \\mathbb{R}^{n \\times m}$, a random matrix with the same shape as $D$.\n",
        "\n",
        "The microdata synthetic generation based on the Cholesky decomposition uses as synthetic dataset the matrix $D'$ constructed as follows:\n",
        "\n",
        "$$D'=RU$$\n",
        "\n",
        "interestingly, while the values in $D'$ are al different from the values in $D$ - thanks to $R$ - their covariance is the same as the original variables."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets test the cholesky decomposition\n",
        "\n",
        "#compute the covariance of the data\n",
        "covm = iris[variables].cov()\n",
        "print(\"This is the computed covariance matrix:\")\n",
        "print(covm)\n",
        "#and the cholesky decomposition of the matrix\n",
        "U = cholesky(covm)\n",
        "print(\"\\nThis is what happens when we reconstruct it after Cholesky decomposition\")\n",
        "print(U.conj().T@U)"
      ],
      "metadata": {
        "id": "90vxQjT9Pjvg"
      },
      "id": "90vxQjT9Pjvg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2113973",
      "metadata": {
        "id": "c2113973"
      },
      "outputs": [],
      "source": [
        "def generation_using_cholesky(dataset: np.array):\n",
        "  \"\"\"\n",
        "  Generates synthetic microdata using the Cholesky decomposition.\n",
        "\n",
        "  This method generates a synthetic dataset with the same covariance structure\n",
        "  as the original dataset, while the individual values are different.\n",
        "\n",
        "  Args:\n",
        "      dataset (np.array): The input numpy array containing numerical data.\n",
        "\n",
        "  Returns:\n",
        "      np.array: A new numpy array containing the synthetically generated data.\n",
        "  \"\"\"\n",
        "\n",
        "  #generate a random matrix R with the same shape as the dataset\n",
        "  R = ???\n",
        "  #compute the covariance of the data\n",
        "  covm = ???\n",
        "  #and the cholesky decomposition of the matrix\n",
        "  U = ???\n",
        "\n",
        "  return R@U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef984a14",
      "metadata": {
        "id": "ef984a14"
      },
      "outputs": [],
      "source": [
        "synthetic_irisdf = iris.copy()\n",
        "synthetic_irisdf[variables] = generation_using_cholesky(synthetic_irisdf[variables].to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48992c00",
      "metadata": {
        "id": "48992c00"
      },
      "outputs": [],
      "source": [
        "synthetic_irisdf[variables].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0daf487",
      "metadata": {
        "id": "f0daf487"
      },
      "outputs": [],
      "source": [
        "iris.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr = synthetic_irisdf[variables].corr()\n",
        "sns.heatmap(corr)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tl4ztId1QVzu"
      },
      "id": "Tl4ztId1QVzu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we still trust the labels?"
      ],
      "metadata": {
        "id": "fYQISvlRQeFU"
      },
      "id": "fYQISvlRQeFU"
    },
    {
      "cell_type": "markdown",
      "id": "c3bbceb6",
      "metadata": {
        "id": "c3bbceb6"
      },
      "source": [
        "<h3>Blank and Impute</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784c6d41",
      "metadata": {
        "id": "784c6d41"
      },
      "outputs": [],
      "source": [
        "#compute the means of each variable\n",
        "means = iris[variables].mean()\n",
        "\n",
        "\n",
        "proportion = 0.5\n",
        "# sample proportion*100% cells (rows and columns)\n",
        "sampled = [(r, c) for r in range(len(iris.index)) for c in range(len(variables)) if npr.random() < proportion]\n",
        "\n",
        "\n",
        "#make a copy of the dataset\n",
        "iris_bi = iris[variables].copy()\n",
        "\n",
        "#for each sampled row, column pair, replace the value with the mean\n",
        "for r, c in sampled:\n",
        "    iris_bi.iloc[r, c] = means[c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b34cc3e2",
      "metadata": {
        "id": "b34cc3e2"
      },
      "outputs": [],
      "source": [
        "iris_bi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b871c965",
      "metadata": {
        "id": "b871c965"
      },
      "outputs": [],
      "source": [
        "iris.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5110123b",
      "metadata": {
        "id": "5110123b"
      },
      "source": [
        "<h2>UNIQUENESS</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e9bf957",
      "metadata": {
        "id": "4e9bf957"
      },
      "outputs": [],
      "source": [
        "considered_variables = ['age', 'workclass', 'education', 'education-num',\n",
        "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n",
        "PU = ???\n",
        "print(f\"population uniqueness: {PU:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5b9f0c9",
      "metadata": {
        "id": "f5b9f0c9"
      },
      "source": [
        "If the population uniqueness is too big, reduce it with microdata protections techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f96d752",
      "metadata": {
        "id": "1f96d752"
      },
      "source": [
        "*SOME SUGGESTIONS FOR HW1*</br>\n",
        "Measure the sample uniqueness of your data. Try also to carry out a simple record linkage analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FITTING A SIMPLE LOGISTIC REGRESSION\n",
        "\n",
        "We now know how to operate on a microdata release to protect it. We also know how to compute the \"risk\" associated with the release in terms of population uniqueness.\n",
        "\n",
        "To carry out a proper privacy analysis, we also need some target utility measure.\n",
        "\n",
        "A possibility, in our case, is to fit a very simple logistic regression to predict the income given the variables in the adult dataset.\n",
        "\n",
        "Your objective for the first HW is not only to apply the techniques mentioned above, but also to show how the accuracy of a (simple) model changes when privacy is applied. Using this information and combining it with the population uniqueness, you can compute an R-U CONFIDENTIALITY MAP."
      ],
      "metadata": {
        "id": "H1HHvLlZR-Fj"
      },
      "id": "H1HHvLlZR-Fj"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "adultdf = pd.read_csv(adult_data_path, sep=\",\", names = adult_columns)\n",
        "adultdf = adultdf.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "\n",
        "# Drop rows with missing values\n",
        "adultdf = adultdf.dropna()\n",
        "\n",
        "# Convert target to binary\n",
        "adultdf['income'] = adultdf['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
        "\n",
        "# Select features and target\n",
        "X = adultdf.drop('income', axis=1)\n",
        "y = adultdf['income']\n",
        "\n",
        "# Encode categorical variables with LabelEncoder\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "7VbM6AJDSELt"
      },
      "id": "7VbM6AJDSELt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CQTkDz2lTr88"
      },
      "id": "CQTkDz2lTr88",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}